# Session 01 – Local setup & your first LLM API call

## Session goal

By the end of this session, you will be able to:

- work locally in a Python project,
- understand what it means to use an LLM via an API,
- securely configure an API key,
- run a first successful request to a Large Language Model from your computer.

This session focuses on **infrastructure and fundamentals**.
We are not building an application yet — we are making sure the foundation works.

---

## What we are (and are not) doing today

### We are doing
- Setting up a local Python environment
- Installing and importing packages
- Understanding how LLM APIs work
- Making a first, minimal API call

### We are not doing
- Prompt engineering
- Model evaluation
- Domain-specific applications
- Qualitative coding or embeddings

Think of this session as **learning how to turn the engine on**.

---

## Big picture: where this fits

Most LLM-based workflows follow the same high-level structure:
```
Input (text, audio, data)
↓
LLM API call
↓
Model output (text, vectors, decisions)
↓
Post-processing and analysis
```

In this session we focus only on the **LLM API call** layer.
Everything else builds on top of it.

---

## Working locally (why this matters)

In many demos, LLMs are shown in notebooks or web interfaces.
For real work, we usually need:

- reproducibility,
- version control,
- explicit dependencies,
- control over data and secrets.

That is why we are working:

- locally,
- in a Git repository,
- using a standard Python project structure.

---

## The repository you are working in

Before continuing, make sure you have cloned this repository and opened it in your editor (VS Code or Positron).

Key folders:

- `docs/` – learning materials and session guides
- `src/` – reusable Python code
- `examples/` – scripts you can run directly
- `data/` – small, non-sensitive input files

For this session, we will mainly use `examples/`.

---

## Python packages and environments

Python relies heavily on external packages.
Even basic tasks often require importing libraries.

In this project:

- package versions are defined in the project configuration,
- a virtual environment isolates those packages from other projects.

You do **not** need to know how this is implemented internally today.
What matters is that everyone runs the same environment.

---

## Creating and activating the environment

If you have not done this already, run:
```bash
just get-started
```

This command:

- installs the correct Python version,
- creates a virtual environment,
- installs required packages.

After that, activate the environment:

| Shell | Command |
|-------|---------|
| Bash | `.venv/Scripts/activate` |
| PowerShell | `.venv/Scripts/activate.ps1` |
| Nushell | `overlay use .venv/Scripts/activate.nu` |

You should now see your terminal indicating an active environment.

---

## What does it mean to "use an LLM"?

Large Language Models do not run on your computer.
They run on remote servers.

When you "use an LLM" from Python, you are:

- sending a request over the internet,
- including text input (your prompt),
- receiving a response generated by the model.

This interaction is handled through an API (Application Programming Interface).

---

## API keys and security

To access an LLM API, you need an API key.

**Important rules:**

- API keys identify you and your account.
- API keys should never be hard-coded in scripts.
- API keys should never be committed to Git.

In this project, API keys are stored as environment variables.

You should have a file called `.env` (not committed to Git) containing:
```
OPENAI_API_KEY=your_key_here
```

Your code reads this key at runtime, without exposing it publicly.

---

## Your first API call

Before doing anything complex, we always test the connection.

The script:
```
examples/01_test_connection.py
```

does exactly one thing:

- sends a minimal request to the LLM,
- prints the response.

Run it from the project root:
```bash
python examples/01_test_connection.py
```

### What should happen

If everything is configured correctly, you should see:

- a confirmation message,
- a short text response from the model.

This means:

- your environment works,
- your API key is correctly set,
- you can successfully communicate with the LLM.

**If this step fails, stop here and fix it before moving on.**
All later steps depend on this working.

---

## Common issues (and what they mean)

**OPENAI_API_KEY not found**
→ your environment variable is not set or not loaded.

**ModuleNotFoundError**
→ your environment is not active or dependencies are not installed.

**Authentication or permission errors**
→ your API key is incorrect or expired.

These issues are normal and part of the setup process.

---

## Next step

Once the connection test works, move on to:
```
examples/02_translate_text.py
```

This script performs a simple, concrete task using the same API connection.
It demonstrates how the same foundation can support many different use cases.

In the next sessions, we will reuse this exact setup to work with:

- longer texts,
- structured outputs,
- embeddings,
- and more complex workflows.