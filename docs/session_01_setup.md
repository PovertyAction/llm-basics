# Session 01 – Local setup & your first LLM API call

## Session goal

By the end of this session, you will be able to:

- work locally in a Python project,
- understand what it means to use an LLM via an API,
- securely configure an API key,
- run a first successful request to a Large Language Model from your computer.

This session focuses on **infrastructure and fundamentals**.
We are not building an application yet, we are making sure the foundation works.

---

## Key concepts

### What does it mean to "use an LLM"?

Large Language Models do not run on your computer.
They run on remote servers.

When you "use an LLM" from Python, you are:

- sending a request over the internet,
- including text input (your prompt),
- receiving a response generated by the model.

This interaction is handled through an **API (Application Programming Interface)**.

**Key insight:** You're not running the model—you're making HTTP requests to a service that runs the model.

---

### The LLM workflow structure

Most LLM-based workflows follow the same high-level structure:

```
Input (text, audio, data)
↓
LLM API call
↓
Model output (text, vectors, decisions)
↓
Post-processing and analysis
```

**In this session we focus only on the LLM API call layer.**
Everything else builds on top of it.

---

### API keys and security

To access an LLM API, you need an API key.

**Important rules:**

- API keys identify you and your account
- API keys should **never** be hard-coded in scripts
- API keys should **never** be committed to Git
- API usage costs money—protect your keys

**How we handle this:**
In this project, API keys are stored as environment variables.

You should have a file called `.env` (not committed to Git) containing:
```
OPENAI_API_KEY=your_key_here
```

Your code reads this key at runtime, without exposing it publicly.

---

### Working locally (why this matters)

In many demos, LLMs are shown in notebooks or web interfaces.

For real work, we usually need:

- **Reproducibility**: Same code produces same results
- **Version control**: Track changes over time
- **Explicit dependencies**: Know exactly what packages you need
- **Control over data and secrets**: Keep sensitive information secure

That is why we are working:

- locally (on your machine),
- in a Git repository,
- using a standard Python project structure.

---

## Guided activity

### Preparation: Understanding the repository

Before continuing, make sure you have cloned this repository and opened it in your editor (VS Code or Positron).

**Key folders:**

- `docs/` – learning materials and session guides
- `src/` – reusable Python code
- `examples/` – scripts you can run directly
- `data/` – small, non-sensitive input files

For this session, we will mainly use `examples/`.

---

### Step 1 — Create and activate the environment (5 min)

Python relies heavily on external packages.
Even basic tasks often require importing libraries.

**In this project:**

- package versions are defined in the project configuration,
- a virtual environment isolates those packages from other projects.

**Run this command:**

```bash
just venv
```

**This command:**

- installs the correct Python version,
- creates a virtual environment,
- installs required packages.

**After that, activate the environment:**

| Shell | Command |
|-------|---------|
| Bash | `.venv/Scripts/activate` |
| PowerShell | `.venv/Scripts/activate.ps1` |
| Nushell | `overlay use .venv/Scripts/activate.nu` |

**What to observe:**

You should now see your terminal indicating an active environment (usually shows `.venv` in the prompt).
(llm-basics)

---

### Step 2 — Test your connection (5 min)

Before doing anything complex, we always test the connection.

**Run this script:**

```bash
python examples/01_test_connection.py
```

**This script does exactly one thing:**

- Sends a minimal request to the LLM
- Prints the response

**What you should see:**
If everything is configured correctly, you should see:

- A confirmation message
- A short text response from the model

**What this confirms:**

- ✅ Your environment works
- ✅ Your API key is correctly set
- ✅ You can successfully communicate with the LLM

**If this step fails, stop here and fix it before moving on.**
All later steps depend on this working.

---

### Step 3 — Run your first real task (10 min)

**Run this script:**
```bash
python examples/02_translate_text.py
```

**This script:**

- Reads a local text file
- Sends it to the LLM with translation instructions
- Prints the model output

**What to observe:**

- The input text (in original language)
- The translated output
- How quickly the response comes back

**What's happening:**

```
1. Script loads text from data/sample_text.txt
2. Script builds a prompt: "Translate this to Spanish: [text]"
3. Script sends prompt to OpenAI API
4. API returns translated text
5. Script prints the result
```

**Key insight:** The same connection pattern works for any task—translation, summarization, analysis, etc.

---

## Mental model: Anatomy of an LLM API call

Every LLM API call has these components:

**1. Client** — authenticated connection
```python
client = get_client()  # Reads your API key
```

**2. Model selection** — which engine to use
```python
model="gpt-4o-mini"  # Different models = different speed/cost/quality
```

**3. Messages/prompt** — the instructions and data
```python
messages=[
  {"role": "system", "content": "You are a helpful assistant"},
  {"role": "user", "content": "Translate this: Hello"}
]
```

**4. API call** — send the request
```python
response = client.chat.completions.create(...)
```

**5. Output parsing** — extract what you need
```python
text = response.choices[0].message.content
```

**Teaching line:** "Client connects, model processes, messages guide, output returns."

---

## Quick exercises (extra)

1. **Modify the task:** Change the translation target language in `02_translate_text.py`

2. **Change the model:** Try a different model like `gpt-4o` (note: different cost/speed)

3. **Add a new task:** Copy `02_translate_text.py` and modify it to summarize instead of translate

4. **Experiment with prompts:** Change the system message and observe how it affects the output

---

## Common issues and solutions

**"OPENAI_API_KEY not found"**

→ Your environment variable is not set or not loaded
→ Check that `.env` file exists and contains your key
→ Make sure you activated the virtual environment

**"ModuleNotFoundError"**

→ Your environment is not active or dependencies are not installed
→ Run `just get-started` again
→ Make sure you see `.venv` in your terminal prompt

**"Authentication or permission errors"**

→ Your API key is incorrect or expired
→ Check your OpenAI account dashboard
→ Generate a new key if needed

**"Script runs but no output"**

→ Check your internet connection
→ Look for error messages in the console
→ Try running `01_test_connection.py` first

These issues are normal and part of the setup process.

---

## What you've learned

✅ How to set up a local Python environment for LLM work
✅ How to securely configure API keys
✅ What an LLM API call actually is (HTTP request to remote service)
✅ How to make your first successful API call
✅ The basic structure of any LLM interaction

---

## Bridge to next sessions

Now that your foundation works, we can build on it:

**Session 02 (next):** Introduction to embeddings and RAG
- Learn what embeddings are
- Understand semantic search
- See how to work with your own documents

**Sessions 03-04 (afternoon):** Practical applications
- Qualitative coding with embeddings
- Building an internal knowledge chatbot

**Key point:** The setup you completed today is reused for everything else. You won't need to do this again.

---

## Key takeaways

1. **LLMs run remotely** — you're making API calls, not running models locally
2. **API keys are secrets** — never commit them, always use environment variables
3. **Local setup matters** — reproducible environments prevent "works on my machine" problems
4. **Same pattern, many uses** — this connection pattern works for any LLM task

---

## What we did (and didn't do)

### ✅ We did

- Set up a local Python environment
- Install and import packages
- Understand how LLM APIs work
- Make a first, minimal API call

### ❌ We did not

- Prompt engineering
- Model evaluation
- Domain-specific applications
- Qualitative coding or embeddings

Think of this session as **learning how to turn the engine on**.
The rest comes next.

---

**Ready for more?** Move on to Session 02 to learn about embeddings and RAG.