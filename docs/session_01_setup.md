# Session 01 ‚Äì Local setup & your first LLM API call

## Session goal

By the end of this session, you will be able to:

- work locally in a Python project,
- understand what it means to use an LLM via an API,
- securely configure an API key,
- run a first successful request to a Large Language Model from your computer,
- understand chat completions and streaming responses,
- explore function calling (tools) with LLMs.

This session focuses on **infrastructure and fundamentals**.
We are not building an application yet ‚Äî we are making sure the foundation works.

---

## Key concepts

### What does it mean to "use an LLM"?

Large Language Models do not run on your computer.
They run on remote servers.

When you "use an LLM" from Python, you are:

- sending a request over the internet,
- including text input (your prompt),
- receiving a response generated by the model.

This interaction is handled through an **API (Application Programming Interface)**.

**Key insight:** You're not running the model‚Äîyou're making HTTP requests to a service that runs the model.

---

### The LLM workflow structure

Most LLM-based workflows follow the same high-level structure:

```text
Input (text, audio, data)
‚Üì
LLM API call
‚Üì
Model output (text, vectors, decisions)
‚Üì
Post-processing and analysis
```

**In this session we focus only on the LLM API call layer.**
Everything else builds on top of it.

---

### API keys and security

To access an LLM API, you need an API key.

**Important rules:**

- API keys identify you and your account
- API keys should **never** be hard-coded in scripts
- API keys should **never** be committed to Git
- API usage costs money‚Äîprotect your keys

**How we handle this:**
In this project, API keys are stored as environment variables.

You should have a file called `.env` (not committed to Git) containing:

```bash
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

Your code reads this key at runtime, without exposing it publicly.

---

### Working locally (why this matters)

In many demos, LLMs are shown in notebooks or web interfaces.

For real work, we usually need:

- **Reproducibility**: Same code produces same results
- **Version control**: Track changes over time
- **Explicit dependencies**: Know exactly what packages you need
- **Control over data and secrets**: Keep sensitive information secure

That is why we are working:

- locally (on your machine),
- in a Git repository,
- using a standard Python project structure.

---

## Guided activity

### Preparation: Understanding the repository

Before continuing, make sure you have cloned this repository and opened it in your editor (VS Code or Positron).

**Key folders:**

- `docs/` ‚Äì learning materials and session guides
- `src/` ‚Äì reusable Python code
- `examples/` ‚Äì scripts you can run directly
- `data/` ‚Äì small, non-sensitive input files

For this session, we will mainly use `examples/`.

---

### Step 1 ‚Äî Create and activate the environment (5 min)

Python relies heavily on external packages.
Even basic tasks often require importing libraries.

**In this project:**

- package versions are defined in the project configuration,
- a virtual environment isolates those packages from other projects.

**Run this command:**

```bash
just venv
```

**This command:**

- installs the correct Python version,
- creates a virtual environment,
- installs required packages.

**After that, activate the environment:**

| Shell | Command |
|-------|---------|
| Bash | `.venv/Scripts/activate` |
| PowerShell | `.venv/Scripts/activate.ps1` |
| Nushell | `overlay use .venv/Scripts/activate.nu` |

**What to observe:**
You should now see your terminal indicating an active environment (usually shows `.venv` in the prompt).

---

### Step 2 ‚Äî Test your connection (5 min)

Before doing anything complex, we always test the connection.

**Run this script:**

```bash
python examples\test_connection.py
```

**This script does exactly one thing:**

- Sends a minimal request to the LLM
- Prints the response

**What you should see:**

If everything is configured correctly, you should see:

- A confirmation message
- A short text response from the model

**What this confirms:**

- ‚úÖ Your environment works
- ‚úÖ Your API key is correctly set
- ‚úÖ You can successfully communicate with the LLM

**If this step fails, stop here and fix it before moving on.**
All later steps depend on this working.

---

## OpenAI Chat Completions

Now that your connection works, we'll explore different patterns for using the Chat Completions API.

These scripts use the `openai` Python package to demonstrate core API features. They are organized in increasing order of complexity:

### Basic chat completions

**1. Simple chat (`chat.py`)**

```bash
python examples/chat.py
```

**What it does:**

- Demonstrates a basic chat completion call
- Sends a single message
- Returns a complete response

**What to observe:**

- The request-response pattern
- How messages are structured
- The model's complete output

---

**2. Streaming responses (`chat_stream.py`)**

```bash
python examples/chat_stream.py
```

**What it does:**

- Adds `stream=True` to the API call
- Returns a generator that streams the completion as it's being generated
- Shows tokens appearing one at a time (like ChatGPT interface)

**What to observe:**

- Response appears progressively
- Better user experience for long responses
- Same final result, different delivery

**When to use streaming:**

- Building chat interfaces
- Long-form content generation
- When you want to show progress to users

---

**3. Chat with history (`chat_history.py`)**

```bash
python examples/chat_history.py
```

**What it does:**

- Creates a back-and-forth chat interface using `input()`
- Keeps track of past messages
- Sends conversation history with each API call

**What to observe:**

- The model "remembers" previous messages
- Context builds up over the conversation
- Each API call includes the full history

**Key insight:** LLMs are stateless‚Äîthe model doesn't remember anything. YOU must send the conversation history each time.

---

**4. Chat with history and streaming (`chat_history_stream.py`)**

```bash
python examples/chat_history_stream.py
```

**What it does:**

- Combines conversation history with streaming
- Most similar to production chatbot interfaces

**What to observe:**

- Full chat experience with progressive responses
- How history management works with streaming

---

## Function calling (Tools)

These scripts demonstrate using the Chat Completions API "tools" feature (also known as function calling).

**What is function calling?**

Instead of only returning text, the model can:

- Decide when to call developer-defined functions
- Return structured arguments matching your function schema
- Let you execute code/APIs based on model decisions

**The workflow:**

1. You declare available functions in the `tools` parameter
2. The model may respond with `message.tool_calls` instead of text
3. Each tool call includes the function `name` and JSON `arguments`
4. Your application executes the function and (optionally) sends results back

---

**1. Basic function calling (`function_calling_basic.py`)**

```bash
python examples/function_calling_basic.py
```

**What it does:**

- Declares a single `lookup_weather` function
- Prompts the model with a weather-related question
- Prints the tool call (if detected) or normal response
- Does NOT actually execute the function

**What to observe:**

- How to declare function schemas
- When the model chooses to call vs. respond normally
- The structure of tool call responses

**Example output:**

```text
Model chose to call: lookup_weather
Arguments: {"city_name": "Bogota"}
```

---

**2. Function calling with execution (`function_calling_call.py`)**

```bash
python examples/function_calling_call.py
```

**What it does:**

- Extends the basic example by ACTUALLY executing the function
- Declares the `lookup_weather` function schema
- When the model requests it, executes the function with provided arguments
- Returns mock weather data (18¬∞C in Celsius)

**What to observe:**

- How to parse tool call arguments from JSON
- How to execute functions based on model decisions
- The complete function calling workflow

**Example output:**

```text
Model chose to call: lookup_weather
Arguments: {"city_name": "Bogota"}

üå§Ô∏è  Looking up weather for Bogota...
Function result: Currently 18¬∞C and partly cloudy
```

**Key insight:** The model decides WHEN to call, YOUR code decides HOW to execute.

---

**3. Document translation (`translate_ipa_document.py`)**

```bash
python examples/translate_ipa_document.py
```

**What it does:**

- Reads the IPA Best Bets document in English (from `data/`)
- Translates the entire document to Spanish using OpenAI
- Saves the Spanish version to `data/ipa-best-bets-2025-es.md`
- Preserves all markdown formatting

**What to observe:**

- How to handle long document translation
- File I/O operations with markdown
- Using lower temperature (0.3) for consistent translation
- Professional translation prompting for academic content

**When to use function calling:**

- When you need structured outputs (JSON, not prose)
- When the model should trigger external actions (API calls, database queries)
- When you want the model to use tools/plugins
- For multi-step workflows (model decides what to do next)

---

## Mental model: Anatomy of an LLM API call

Every LLM API call has these components:

**1. Client** ‚Äî authenticated connection

```python
client = get_client()  # Reads your API key
```

**2. Model selection** ‚Äî which engine to use

```python
model="gpt-4o-mini"  # Different models = different speed/cost/quality
```

**3. Messages/prompt** ‚Äî the instructions and data

```python
messages=[
  {"role": "system", "content": "You are a helpful assistant"},
  {"role": "user", "content": "Hello"}
]
```

**4. Optional parameters** ‚Äî control behavior

```python
stream=True,  # Stream responses
tools=[...],  # Enable function calling
temperature=0.7,  # Control randomness
```

**5. API call** ‚Äî send the request

```python
response = client.chat.completions.create(...)
```

**6. Output parsing** ‚Äî extract what you need

```python
# For normal responses:
text = response.choices[0].message.content

# For tool calls:
tool_calls = response.choices[0].message.tool_calls
```

---

## Quick exercises (After the training session)

**Chat completions:**

1. Modify `chat.py` to ask a different question
2. Compare response times between `chat.py` and `chat_stream.py`
3. In `chat_history.py`, observe how context affects responses

**Function calling:**

1. Compare `function_calling_basic.py` vs `function_calling_call.py` ‚Äî what's the difference?
2. Modify the question in `function_calling_call.py` to ask about a different city
3. Add a new parameter to `lookup_weather` (e.g., `forecast_days`)
4. Test what happens when you ask a question that shouldn't trigger the function

---

## Common issues and solutions

### "OPENAI_API_KEY not found"

‚Üí Your environment variable is not set or loaded  
‚Üí Check that `.env` file exists and contains your key  
‚Üí Make sure you activated the virtual environment

### "ModuleNotFoundError: No module named 'openai'"

‚Üí Your environment is not active or dependencies are not installed  
‚Üí Run `just get-started` again  
‚Üí Make sure you see `.venv` in your terminal prompt

### "Streaming doesn't work / no output"

‚Üí Check you're iterating over the stream correctly  
‚Üí Look for `for chunk in response:` pattern

### "Function never gets called"

‚Üí Check your function schema matches what the model expects  
‚Üí Make your prompt clearer about when to use the function  
‚Üí Try different prompts that clearly need the function

### "Tool call arguments are wrong"

‚Üí Check your function description is clear  
‚Üí Verify parameter descriptions explain what's expected  
‚Üí The model infers from descriptions‚Äîbe specific

---

## What you've learned

‚úÖ How to set up a local Python environment for LLM work
‚úÖ How to securely configure API keys
‚úÖ What an LLM API call actually is (HTTP request to remote service)
‚úÖ How to make basic chat completions
‚úÖ How streaming works and when to use it
‚úÖ How to maintain conversation history
‚úÖ What function calling is and when it's useful
‚úÖ The basic structure of any LLM interaction

---

## Key patterns to remember

**1. Basic completion:** Simple question ‚Üí answer
**2. Streaming:** Progressive response delivery
**3. Conversation history:** YOU manage context, model is stateless
**4. Function calling:** Model decides when to use tools, YOU execute them

---

## Bridge to next sessions

Now that your foundation works, we can build on it:

**Session 02 (next):** Introduction to embeddings and RAG

- Learn what embeddings are
- Understand semantic search
- See how to work with your own documents

**Sessions 03-04 (afternoon):** Practical applications

- Qualitative coding with embeddings
- Building an internal knowledge chatbot

**Key point:** The setup you completed today is reused for everything else. You won't need to do this again.

---

## What we did (and didn't do)

### ‚úÖ We did

- Set up a local Python environment
- Understand how LLM APIs work
- Make basic and streaming chat completions
- Manage conversation history
- Explore function calling (tools)

### ‚ùå We did not

- Advanced prompt engineering
- Model evaluation
- Production deployment
- Complex multi-step workflows
- Embeddings and RAG (that's next)

Think of this session as **learning the core API patterns**.
Everything else builds on these fundamentals.

---

## Acknowledgments

Some exercises and examples in this session were adapted from or inspired by the [Azure Python OpenAI Samples](https://github.com/Azure-Samples/python-openai-demos) repository.

---

**Ready for more?** Move on to Session 02 to learn about embeddings and RAG.
