# Sesi√≥n 01 ‚Äì Configuraci√≥n local y primera llamada a una LLM API

## Objetivo de la sesi√≥n

Al final de esta sesi√≥n, deben poder:

- trabajar localmente en un proyecto de Python,
- entender qu√© significa usar un LLM a trav√©s de una API,
- configurar de forma segura una clave de API,
- ejecutar una primera solicitud exitosa a un Modelo de lenguaje de gran tama√±o desde el computador,
- entender las respuestas de chat y el streaming,
- explorar la llamada a funciones (tools) con LLMs.

Esta sesi√≥n se enfoca en **infraestructura y fundamentos**.
No estamos construyendo una aplicaci√≥n todav√≠a ‚Äî estamos asegur√°ndonos de que la base funcione.

---

## Conceptos clave

### ¬øQu√© significa "usar un LLM"?

Los Modelos de lenguaje de gran tama√±o no se ejecutan en el computador.
Se ejecutan en servidores remotos.

Cuando "usas un LLM" desde Python, est√°s:

- enviando una solicitud por internet,
- incluyendo entrada de texto (tu prompt),
- recibiendo una respuesta generada por el modelo.
s
Esta interacci√≥n se maneja a trav√©s de una **API (Interfaz de Programaci√≥n de Aplicaciones)**.

**Punto clave:** No est√°s ejecutando el modelo‚Äîest√°s haciendo solicitudes HTTP a un servicio que ejecuta el modelo.

---

### La estructura del flujo de trabajo con LLM

La mayor√≠a de los flujos de trabajo basados en LLM siguen la misma estructura de alto nivel:

```text
Entrada (texto, audio, datos)
‚Üì
Llamada a la API del LLM
‚Üì
Salida del modelo (texto, vectores, decisiones)
‚Üì
Post-procesamiento y an√°lisis
```

**En esta sesi√≥n nos enfocamos solo en la capa de llamada a la API del LLM.**
Todo lo dem√°s se construye sobre esto.

---

### Claves de API y seguridad

Para acceder a una API de LLM, necesitas una clave de API.

**Reglas importantes:**

- Las claves de API te identifican a ti y a tu cuenta
- Las claves de API **nunca** deben codificarse directamente en los scripts
- Las claves de API **nunca** deben confirmarse en Git
- El uso de la API cuesta dinero‚Äîprotege tus claves

**C√≥mo manejamos esto:**
En este proyecto, las claves de API se almacenan como variables de entorno.

Deber√≠as tener un archivo llamado `.env` (no confirmado en Git) que contenga:

```bash
# Clave de API de Anthropic (recomendada)
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Clave de API de OpenAI (opcional - si quieres usar OpenAI)
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

**Obtener una clave de API de Anthropic:**

1. Ve a [https://console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys)
2. Reg√≠strate o inicia sesi√≥n en tu cuenta
3. Crea una nueva clave de API
4. Copia la clave (comienza con `sk-ant-`)
5. Agr√©gala a tu archivo `.env` como se muestra arriba

**Obtener una clave de API de OpenAI:**

1. Ve a [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
2. Reg√≠strate o inicia sesi√≥n en tu cuenta
3. Crea una nueva clave de API (puede requerir agregar un m√©todo de pago)
4. Copia la clave (comienza con `sk-proj-`)
5. Agr√©gala a tu archivo `.env` como se muestra arriba

**Selecci√≥n de proveedor:**

- Si solo `ANTHROPIC_API_KEY` est√° configurada ‚Üí usa Anthropic (modelos Claude)
- Si solo `OPENAI_API_KEY` est√° configurada ‚Üí usa OpenAI (modelos GPT)
- Si ambas est√°n configuradas ‚Üí usa Anthropic por defecto
- Configura `LLM_PROVIDER=openai` o `LLM_PROVIDER=anthropic` para elegir expl√≠citamente

Tu c√≥digo lee estas claves en tiempo de ejecuci√≥n, sin exponerlas p√∫blicamente.

---

### Trabajar localmente (por qu√© esto importa)

En muchas demostraciones, los LLMs se muestran en notebooks o interfaces web.

Para trabajo real, usualmente necesitamos:

- **Reproducibilidad**: El mismo c√≥digo produce los mismos resultados
- **Control de versiones**: Rastrear cambios a lo largo del tiempo
- **Dependencias expl√≠citas**: Saber exactamente qu√© paquetes necesitas
- **Control sobre datos y PII**: Mantener la informaci√≥n sensible segura

Por eso estamos trabajando:

- localmente (en el computador),
- en un repositorio de Git,
- usando una estructura de proyecto de Python est√°ndar.

---

## Actividad guiada

### Preparaci√≥n: Familiarizarse el repositorio

Antes de continuar, aseg√∫rate de haber clonado este repositorio y abierto en tu editor (VS Code o Positron).

**Carpetas clave:**

- `docs/` ‚Äì materiales de aprendizaje y gu√≠as de sesiones
- `src/` ‚Äì c√≥digo Python reutilizable
- `examples/` ‚Äì scripts que puedes ejecutar directamente
- `data/` ‚Äì archivos de entrada peque√±os, no sensibles

Para esta sesi√≥n, usaremos principalmente `examples/`.

---

### Paso 1 ‚Äî Crear y activar el entorno (5 min)

Python depende mucho de paquetes externos.
Incluso tareas b√°sicas a menudo requieren importar bibliotecas.

**En este proyecto:**

- las versiones de los paquetes est√°n definidas en la configuraci√≥n del proyecto,
- un entorno virtual a√≠sla esos paquetes de otros proyectos.

**Ejecuta este comando:**

Si es la primera vez que configuras el proyecto, ejecuta:

```bash
just get-started
```

Si ya has configurado el proyecto antes, solo ejecuta:

```bash
just venv
```

**Este comando:**

- instala la versi√≥n correcta de Python,
- crea un entorno virtual,
- instala los paquetes requeridos.

**Despu√©s de eso, activa el entorno:**

| Shell      | Comando                                     |
|------------|---------------------------------------------|
| Bash       | `.venv/Scripts/activate`                    |
| PowerShell | `.venv/Scripts/activate.ps1`                |
| Nushell    | `overlay use .venv/Scripts/activate.nu`     |

**Qu√© observar:**
Ahora deber√≠as ver tu terminal indicando un entorno activo (usualmente muestra `.venv` en el prompt).

---

### Paso 2 ‚Äî Prueba tu conexi√≥n (5 min)

Antes de hacer algo complejo, siempre probamos la conexi√≥n.

**Ejecuta este script:**

```bash
python examples\test_connection.py
```

**Este script hace exactamente una cosa:**

- Env√≠a una solicitud m√≠nima al LLM
- Imprime la respuesta

**Qu√© deber√≠as ver:**

Si todo est√° configurado correctamente, deber√≠as ver:

- Un mensaje de confirmaci√≥n
- Una respuesta de texto corta del modelo

**Qu√© confirma esto:**

- ‚úÖ Tu entorno funciona
- ‚úÖ Tu clave de API est√° configurada correctamente
- ‚úÖ Puedes comunicarte exitosamente con el LLM

**Si este paso falla, detente aqu√≠ y arr√©glalo antes de continuar.**
Todos los pasos posteriores dependen de que esto funcione.

---

## Llamadas de chat LLM

Ahora que tu conexi√≥n funciona, exploraremos diferentes patrones para usar la API de Chat Completions.

Estos scripts funcionan tanto con modelos de Anthropic (Claude) como de OpenAI (GPT), detectando autom√°ticamente qu√© proveedor usar seg√∫n tus claves de API. Est√°n organizados en orden creciente de complejidad:

### Llamadas b√°sicas de chat

**1. Chat simple (`chat.py`)**

```bash
python examples/chat.py
```

**Qu√© hace:**

- Demuestra una llamada b√°sica de chat
- Env√≠a un solo mensaje
- Devuelve una respuesta completa

**Qu√© observar:**

- El patr√≥n de solicitud-respuesta
- C√≥mo se estructuran los mensajes
- La salida completa del modelo

---

**2. Respuestas en streaming (`chat_stream.py`)**

```bash
python examples/chat_stream.py
```

**Qu√© hace:**

- Agrega `stream=True` a la llamada de API
- Devuelve un generador que transmite la respuesta a medida que se genera
- Muestra tokens apareciendo uno a la vez (como la interfaz de ChatGPT)

**Qu√© observar:**

- La respuesta aparece progresivamente
- Mejor experiencia de usuario para respuestas largas
- Mismo resultado final, entrega diferente

**Cu√°ndo usar streaming:**

- Construir interfaces de chat
- Generaci√≥n de contenido largo
- Cuando quieres mostrar progreso a los usuarios

---

**3. Chat con historial (`chat_history.py`)**

```bash
python examples/chat_history.py
```

**Qu√© hace:**

- Crea una interfaz de chat de ida y vuelta usando `input()`
- Rastrea mensajes pasados
- Env√≠a el historial de conversaci√≥n con cada llamada a la API

**Qu√© observar:**

- El modelo "recuerda" mensajes anteriores
- El contexto se acumula durante la conversaci√≥n
- Cada llamada a la API incluye el historial completo

**Punto clave:** Los LLMs son sin estado‚Äîel modelo no recuerda nada. T√ö debes enviar el historial de conversaci√≥n cada vez.

---

**4. Chat con historial y streaming (`chat_history_stream.py`)**

```bash
python examples/chat_history_stream.py
```

**Qu√© hace:**

- Combina el historial de conversaci√≥n con streaming
- M√°s similar a interfaces de chatbot de producci√≥n

**Qu√© observar:**

- Experiencia de chat completa con respuestas progresivas
- C√≥mo funciona la gesti√≥n del historial con streaming

---

## Llamada a funciones (Tools)

Estos scripts demuestran el uso de la funci√≥n "tools" de la API de Chat Completions (tambi√©n conocida como llamada a funciones).

**¬øQu√© es la llamada a funciones?**

En lugar de solo devolver texto, el modelo puede:

- Decidir cu√°ndo llamar funciones definidas por el desarrollador
- Devolver argumentos estructurados que coincidan con el esquema de tu funci√≥n
- Permitirte ejecutar c√≥digo/APIs bas√°ndose en decisiones del modelo

**El flujo de trabajo:**

1. Declaras las funciones disponibles en el par√°metro `tools`
2. El modelo puede responder con `message.tool_calls` en lugar de texto
3. Cada llamada a herramienta incluye el `name` de la funci√≥n y los `arguments` JSON
4. Tu aplicaci√≥n ejecuta la funci√≥n y (opcionalmente) env√≠a los resultados de vuelta

---

**1. Llamada a funciones b√°sica (`function_calling_basic.py`)**

```bash
python examples/function_calling_basic.py
```

**Qu√© hace:**

- Declara una sola funci√≥n `lookup_weather`
- Pregunta al modelo con una pregunta relacionada con el clima
- Imprime la llamada a herramienta (si se detecta) o la respuesta normal
- NO ejecuta realmente la funci√≥n

**Qu√© observar:**

- C√≥mo declarar esquemas de funciones
- Cu√°ndo el modelo elige llamar vs. responder normalmente
- La estructura de las respuestas de llamadas a herramientas

**Ejemplo de salida:**

```text
Model chose to call: lookup_weather
Arguments: {"city_name": "Bogota"}
```

---

**2. Llamada a funciones con ejecuci√≥n (`function_calling_call.py`)**

```bash
python examples/function_calling_call.py
```

**Qu√© hace:**

- Extiende el ejemplo b√°sico EJECUTANDO REALMENTE la funci√≥n
- Declara el esquema de la funci√≥n `lookup_weather`
- Cuando el modelo lo solicita, ejecuta la funci√≥n con los argumentos proporcionados
- Devuelve datos meteorol√≥gicos simulados (18¬∞C en Celsius)

**Qu√© observar:**

- C√≥mo analizar argumentos de llamadas a herramientas desde JSON
- C√≥mo ejecutar funciones bas√°ndose en decisiones del modelo
- El flujo de trabajo completo de llamada a funciones

**Ejemplo de salida:**

```text
Model chose to call: lookup_weather
Arguments: {"city_name": "Bogota"}

üå§Ô∏è  Looking up weather for Bogota...
Function result: Currently 18¬∞C and partly cloudy
```

**Punto clave:** El modelo decide CU√ÅNDO llamar, TU c√≥digo decide C√ìMO ejecutar.

---

**3. Traducci√≥n de documentos (`translate_ipa_document.py`)**

```bash
python examples/translate_ipa_document.py
```

**Qu√© hace:**

- Lee el documento de IPA Best Bets en ingl√©s (desde `data/`)
- Traduce todo el documento al espa√±ol usando el LLM configurado
- Guarda la versi√≥n en espa√±ol en `data/ipa-best-bets-2025-es.md`
- Preserva todo el formato markdown

**Qu√© observar:**

- C√≥mo manejar la traducci√≥n de documentos largos
- Operaciones de E/S de archivos con markdown
- Usar temperatura m√°s baja (0.3) para traducci√≥n consistente
- Prompting de traducci√≥n profesional para contenido acad√©mico

**Cu√°ndo usar llamada a funciones:**

- Cuando necesitas salidas estructuradas (JSON, no prosa)
- Cuando el modelo debe desencadenar acciones externas (llamadas a API, consultas a base de datos)
- Cuando quieres que el modelo use herramientas/plugins
- Para flujos de trabajo de m√∫ltiples pasos (el modelo decide qu√© hacer a continuaci√≥n)

---

## Modelo mental: Anatom√≠a de una llamada a la API de LLM

Cada llamada a la API de LLM tiene estos componentes:

**1. Cliente** ‚Äî conexi√≥n autenticada

```python
client = get_client()  # Lee tu clave de API
```

**2. Selecci√≥n de modelo** ‚Äî qu√© motor usar

```python
# Proveedor autodetectado desde tus claves de API
provider = get_provider()  # Devuelve "anthropic" o "openai"

# Seleccionar modelo apropiado para el proveedor
model = "gpt-4o-mini" if provider == "openai" else "claude-haiku-4-5"
```

**3. Mensajes/prompt** ‚Äî las instrucciones y datos

```python
messages=[
  {"role": "system", "content": "Eres un asistente √∫til"},
  {"role": "user", "content": "Hola"}
]
```

**4. Par√°metros opcionales** ‚Äî controlar comportamiento

```python
stream=True,  # Transmitir respuestas
tools=[...],  # Habilitar llamada a funciones
temperature=0.7,  # Controlar aleatoriedad
```

**5. Llamada a la API** ‚Äî enviar la solicitud

```python
# Usando funciones adaptadoras para soporte multi-proveedor
response = create_completion(
    client=client,
    provider=provider,
    model=model,
    messages=messages,
    temperature=0.7
)
```

**6. An√°lisis de salida** ‚Äî extraer lo que necesitas

```python
# Las funciones adaptadoras devuelven texto normalizado
text = create_completion(...)  # Devuelve string directamente

# Para llamadas a herramientas, usa el helper extract_tool_calls
tool_calls = extract_tool_calls(response, provider)
```

---

## Ejercicios r√°pidos (Despu√©s de la sesi√≥n de entrenamiento)

**Llamadas de chat:**

1. Modifica `chat.py` para hacer una pregunta diferente
2. Compara tiempos de respuesta entre `chat.py` y `chat_stream.py`
3. En `chat_history.py`, observa c√≥mo el contexto afecta las respuestas

**Llamada a funciones:**

1. Compara `function_calling_basic.py` vs `function_calling_call.py` ‚Äî ¬øcu√°l es la diferencia?
2. Modifica la pregunta en `function_calling_call.py` para preguntar sobre una ciudad diferente
3. Agrega un nuevo par√°metro a `lookup_weather` (ej., `forecast_days`)
4. Prueba qu√© sucede cuando haces una pregunta que no deber√≠a activar la funci√≥n

---

## Problemas comunes y soluciones

### "No API keys found" o "ANTHROPIC_API_KEY not found"

‚Üí Tu variable de entorno no est√° configurada o cargada
‚Üí Verifica que el archivo `.env` existe y contiene tu clave de API
‚Üí Aseg√∫rate de haber activado el entorno virtual
‚Üí Verifica el formato de la clave: las claves de Anthropic comienzan con `sk-ant-`, las de OpenAI con `sk-proj-`

### "ModuleNotFoundError: No module named 'anthropic'" o "'openai'"

‚Üí Tu entorno no est√° activo o las dependencias no est√°n instaladas
‚Üí Ejecuta `just get-started` nuevamente
‚Üí Aseg√∫rate de ver `.venv` en tu prompt de terminal

### "¬øQu√© proveedor se est√° usando?"

‚Üí Cada script imprime qu√© proveedor y modelo est√° usando
‚Üí Busca salida como "Using anthropic with model: claude-haiku-4-5"
‚Üí Configura la variable de entorno `LLM_PROVIDER` para elegir expl√≠citamente

### "Streaming no funciona / sin salida"

‚Üí Verifica que est√°s iterando sobre el stream correctamente
‚Üí Busca el patr√≥n `for chunk in response:`

### "La funci√≥n nunca se llama"

‚Üí Verifica que tu esquema de funci√≥n coincida con lo que el modelo espera
‚Üí Haz tu prompt m√°s claro sobre cu√°ndo usar la funci√≥n
‚Üí Prueba diferentes prompts que necesiten claramente la funci√≥n

### "Los argumentos de la llamada a herramienta son incorrectos"

‚Üí Verifica que la descripci√≥n de tu funci√≥n sea clara
‚Üí Verifica que las descripciones de par√°metros expliquen lo que se espera
‚Üí El modelo infiere de las descripciones‚Äîs√© espec√≠fico

---

## Lo que has aprendido

- ‚úÖ C√≥mo configurar un entorno Python local para trabajo con LLM
- ‚úÖ C√≥mo configurar claves de API de forma segura
- ‚úÖ Qu√© es realmente una llamada a la API de LLM (solicitud HTTP a servicio remoto)
- ‚úÖ C√≥mo hacer llamadas de chat b√°sicas
- ‚úÖ C√≥mo funciona el streaming y cu√°ndo usarlo
- ‚úÖ C√≥mo mantener el historial de conversaci√≥n
- ‚úÖ Qu√© es la llamada a funciones y cu√°ndo es √∫til
- ‚úÖ La estructura b√°sica de cualquier interacci√≥n con LLM

---

## Patrones clave para recordar

**1. Llamada b√°sica:** Pregunta simple ‚Üí respuesta
**2. Streaming:** Entrega progresiva de respuesta
**3. Historial de conversaci√≥n:** T√ö gestionas el contexto, el modelo es sin estado
**4. Llamada a funciones:** El modelo decide cu√°ndo usar herramientas, T√ö las ejecutas

---

## Puente a las pr√≥ximas sesiones

Ahora que tu base funciona, podemos construir sobre ella:

**Sesi√≥n 02 (siguiente):** Introducci√≥n a embeddings y RAG

- Aprende qu√© son los embeddings
- Comprende la b√∫squeda sem√°ntica
- Ve c√≥mo trabajar con tus propios documentos

**Sesiones 03-04 (tarde):** Aplicaciones pr√°cticas

- Codificaci√≥n cualitativa con embeddings
- Construcci√≥n de un chatbot de conocimiento interno

**Punto clave:** La configuraci√≥n que completaste hoy se reutiliza para todo lo dem√°s. No necesitar√°s hacer esto nuevamente.

---

## Qu√© hicimos (y qu√© no hicimos)

### ‚úÖ Hicimos

- Configurar un entorno Python local
- Entender c√≥mo funcionan las APIs de LLM
- Hacer llamadas de chat b√°sicas y en streaming
- Gestionar el historial de conversaci√≥n
- Explorar la llamada a funciones (tools)

### ‚ùå No hicimos

- Ingenier√≠a de prompts avanzada
- Evaluaci√≥n de modelos
- Despliegue en producci√≥n
- Flujos de trabajo complejos de m√∫ltiples pasos
- Embeddings y RAG (eso es lo siguiente)

Piensa en esta sesi√≥n como **aprender los patrones b√°sicos de API**.
Todo lo dem√°s se construye sobre estos fundamentos.

---

## Soporte multi-proveedor

Todos los ejemplos de esta sesi√≥n soportan tanto modelos de Anthropic (Claude) como de OpenAI (GPT):

**C√≥mo funciona:**

- Los scripts detectan autom√°ticamente qu√© clave de API has configurado
- Las diferencias espec√≠ficas del proveedor (formato de API, par√°metros) se manejan de forma transparente
- Puedes cambiar de proveedor cambiando qu√© clave de API est√° configurada en `.env`

**Modelos soportados:**

- **Anthropic:** claude-haiku-4-5 (usado en todos los ejemplos)
- **OpenAI:** gpt-4o-mini (r√°pido, rentable) y gpt-4o (mayor calidad)

**¬øPor qu√© multi-proveedor?**

- Comparar diferentes modelos para tu caso de uso
- Evitar dependencia de un proveedor
- Aprender patrones de API que funcionan entre proveedores
- Educativo: Ver c√≥mo funcionan diferentes APIs de LLM

---

## Agradecimientos

Algunos ejercicios y ejemplos en esta sesi√≥n fueron adaptados o inspirados por el repositorio [Azure Python OpenAI Samples](https://github.com/Azure-Samples/python-openai-demos).

Los ejemplos soportan tanto APIs de Anthropic como de OpenAI con detecci√≥n autom√°tica de proveedor.

---

**¬øListo para m√°s?** Pasa a la Sesi√≥n 02 para aprender sobre embeddings y RAG.
